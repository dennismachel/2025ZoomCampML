{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55340e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preparation Stage now Complete ---\n",
      "Total missing values after imputation: 0\n",
      "\n",
      "--- Question 1: Mode for 'industry' ---\n",
      "The most frequent observation (mode) for 'industry' is: retail\n",
      "\n",
      "--- Question 2: Correlation Matrix ---\n",
      "The two features with the biggest absolute correlation are: annual_income and interaction_count (0.0270)\n",
      "\n",
      "--- Data Split (Seed 42) ---\n",
      "Train size: 877 | Val size: 292 | Test size: 293\n",
      "\n",
      "--- Question 3: Mutual Information Score ---\n",
      "Mutual Information Scores (Rounded to 2 decimals):\n",
      "lead_source          0.04\n",
      "industry             0.03\n",
      "employment_status    0.02\n",
      "location             0.02\n",
      "dtype: float64\n",
      "Variable with the biggest mutual information score: lead_source\n",
      "\n",
      "--- Question 4: Base Logistic Regression Accuracy ---\n",
      "Accuracy on validation dataset: 0.7432\n",
      "Accuracy rounded to 2 decimals: 0.74\n",
      "\n",
      "--- Question 5: Feature Elimination ---\n",
      "  Accuracy without 'industry': 0.7432 | Difference: 0.00000\n",
      "  Accuracy without 'employment_status': 0.7466 | Difference: -0.00342\n",
      "  Accuracy without 'lead_score': 0.7432 | Difference: 0.00000\n",
      "Feature with the smallest difference: 'employment_status'\n",
      "Smallest difference value: -0.00342\n",
      "\n",
      "--- Question 6: Regularized Logistic Regression (C Tunning) ---\n",
      "Accuracy scores for different C values (rounded to 3 decimals):\n",
      "{0.01: 0.743, 0.1: 0.743, 1: 0.743, 10: 0.743, 100: 0.743}\n",
      "Best C (smallest C for best accuracy): 0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Load Data in PD DataFrame\n",
    "df = pd.read_csv('course_lead_scoring.txt', sep=',')\n",
    "\n",
    "#define target and features\n",
    "target = 'converted'\n",
    "y = df[target]\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "#Identify categorical and numerical columns\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].fillna('NA')\n",
    "\n",
    "# numerical features with 0.0\n",
    "for col in numerical_cols:\n",
    "    X[col] = X[col].fillna(0.0)\n",
    "\n",
    "print(\"--- Data Preparation Stage now Complete ---\")\n",
    "print(f\"Total missing values after imputation: {X.isnull().sum().sum()}\")\n",
    "\n",
    "#Question 1: Mode for industry\n",
    "print(\"\\n--- Question 1: Mode for 'industry' ---\")\n",
    "# The mode is calculated after filling NAs, as 'NA' itself can be the mode.\n",
    "mode_industry = X['industry'].mode()[0]\n",
    "print(f\"The most frequent observation (mode) for 'industry' is: {mode_industry}\")\n",
    "\n",
    "#Question 2: Correllation Matrix\n",
    "print(\"\\n--- Question 2: Correlation Matrix ---\")\n",
    "correlation_matrix = X[numerical_cols].corr()\n",
    "\n",
    "pairs_to_check = [\n",
    "    ('interaction_count', 'lead_score'),\n",
    "    ('number_of_courses_viewed', 'lead_score'),\n",
    "    ('number_of_courses_viewed', 'interaction_count'),\n",
    "    ('annual_income', 'interaction_count')\n",
    "]\n",
    "\n",
    "max_corr = -1\n",
    "max_pair = None\n",
    "\n",
    "for f1, f2 in pairs_to_check:\n",
    "    corr = correlation_matrix.loc[f1, f2]\n",
    "    if abs(corr) > max_corr:\n",
    "        max_corr = abs(corr)\n",
    "        max_pair = (f1, f2)\n",
    "\n",
    "print(f\"The two features with the biggest absolute correlation are: {max_pair[0]} and {max_pair[1]} ({max_corr:.4f})\")\n",
    "\n",
    "# --- Data split (60%/20%/20%, Seed 42) ---\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\\n--- Data Split (Seed 42) ---\")\n",
    "print(f\"Train size: {len(X_train)} | Val size: {len(X_val)} | Test size: {len(X_test)}\")\n",
    "\n",
    "# --- 5. Question 3: Mutual Information Score ---\n",
    "print(\"\\n--- Question 3: Mutual Information Score ---\")\n",
    "\n",
    "def calculate_mutual_info(X_cat, y):\n",
    "    # Mutual Info requires converting string categories to integers\n",
    "    X_encoded = pd.DataFrame()\n",
    "    for col in X_cat.columns:\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_cat[col])\n",
    "    \n",
    "    mi_scores = mutual_info_classif(X_encoded, y, random_state=42)\n",
    "    scores = pd.Series(mi_scores, index=X_cat.columns)\n",
    "    return scores\n",
    "\n",
    "mi_scores_train = calculate_mutual_info(X_train[categorical_cols], y_train)\n",
    "mi_scores_train_rounded = round(mi_scores_train, 2).sort_values(ascending=False)\n",
    "\n",
    "print(\"Mutual Information Scores (Rounded to 2 decimals):\")\n",
    "print(mi_scores_train_rounded)\n",
    "print(f\"Variable with the biggest mutual information score: {mi_scores_train_rounded.index[0]}\")\n",
    "\n",
    "\n",
    "# --- 6. Preparation for Logistic Regression (One-Hot Encoding) ---\n",
    "\n",
    "# One-hot encode categorical features for all sets\n",
    "X_train_processed = pd.get_dummies(X_train, columns=categorical_cols, drop_first=False)\n",
    "X_val_processed = pd.get_dummies(X_val, columns=categorical_cols, drop_first=False)\n",
    "X_test_processed = pd.get_dummies(X_test, columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# Align columns to ensure consistency across sets (important for model input)\n",
    "common_cols = list(set(X_train_processed.columns) & set(X_val_processed.columns) & set(X_test_processed.columns))\n",
    "X_train_processed = X_train_processed[common_cols]\n",
    "X_val_processed = X_val_processed[common_cols]\n",
    "X_test_processed = X_test_processed[common_cols]\n",
    "\n",
    "# Re-check columns that were not common and add them as 0s if missing in other sets\n",
    "train_cols = X_train_processed.columns\n",
    "for df_set in [X_val_processed, X_test_processed]:\n",
    "    for col in train_cols:\n",
    "        if col not in df_set.columns:\n",
    "            df_set[col] = 0\n",
    "\n",
    "# Final alignment\n",
    "X_val_processed = X_val_processed[train_cols]\n",
    "X_test_processed = X_test_processed[train_cols]\n",
    "\n",
    "# --- 7. Question 4: Base Logistic Regression Accuracy (C=1.0) ---\n",
    "print(\"\\n--- Question 4: Base Logistic Regression Accuracy ---\")\n",
    "\n",
    "model_base = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model_base.fit(X_train_processed, y_train)\n",
    "y_pred_val_base = model_base.predict(X_val_processed)\n",
    "accuracy_base = accuracy_score(y_val, y_pred_val_base)\n",
    "accuracy_base_rounded = round(accuracy_base, 2)\n",
    "\n",
    "print(f\"Accuracy on validation dataset: {accuracy_base:.4f}\")\n",
    "print(f\"Accuracy rounded to 2 decimals: {accuracy_base_rounded}\")\n",
    "\n",
    "\n",
    "# ---Question 5:\n",
    "print(\"\\n--- Question 5: Feature Elimination ---\")\n",
    "\n",
    "features_to_eliminate = ['industry', 'employment_status', 'lead_score']\n",
    "accuracy_original = accuracy_base # Use unrounded base accuracy\n",
    "difference_scores = {}\n",
    "\n",
    "for feature in features_to_eliminate:\n",
    "    # 1. Identify columns related to the feature (OHE categories or numerical)\n",
    "    if feature in numerical_cols:\n",
    "        cols_to_drop = [feature]\n",
    "    else: # Categorical feature (drop all OHE columns starting with the name)\n",
    "        cols_to_drop = [col for col in X_train_processed.columns if col.startswith(f'{feature}_')]\n",
    "\n",
    "    # 2. Drop features and train new model\n",
    "    X_train_elim = X_train_processed.drop(columns=cols_to_drop)\n",
    "    X_val_elim = X_val_processed.drop(columns=cols_to_drop)\n",
    "\n",
    "    model_elim = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    model_elim.fit(X_train_elim, y_train)\n",
    "\n",
    "    # 3. Predict and calculate accuracy\n",
    "    accuracy_elim = accuracy_score(y_val, model_elim.predict(X_val_elim))\n",
    "\n",
    "    # 4. Calculate difference: Original Accuracy - New Accuracy\n",
    "    difference = accuracy_original - accuracy_elim\n",
    "    difference_scores[feature] = difference\n",
    "\n",
    "    print(f\"  Accuracy without '{feature}': {accuracy_elim:.4f} | Difference: {difference:.5f}\")\n",
    "\n",
    "# Find the feature with the smallest difference (closest to 0, potentially negative)\n",
    "smallest_diff_feature = min(difference_scores, key=lambda k: difference_scores[k])\n",
    "smallest_diff = difference_scores[smallest_diff_feature]\n",
    "\n",
    "print(f\"Feature with the smallest difference: '{smallest_diff_feature}'\")\n",
    "print(f\"Smallest difference value: {smallest_diff:.5f}\")\n",
    "\n",
    "\n",
    "# Question 6: Regularized Logistic Regression (C Tunning) ---\n",
    "print(\"\\n--- Question 6: Regularized Logistic Regression (C Tunning) ---\")\n",
    "\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "accuracy_scores = {}\n",
    "\n",
    "for C in C_values:\n",
    "    model_reg = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
    "    model_reg.fit(X_train_processed, y_train)\n",
    "    y_pred_val_reg = model_reg.predict(X_val_processed)\n",
    "    accuracy_reg = accuracy_score(y_val, y_pred_val_reg)\n",
    "    accuracy_scores[C] = round(accuracy_reg, 3)\n",
    "\n",
    "print(\"Accuracy scores for different C values (rounded to 3 decimals):\")\n",
    "print(accuracy_scores)\n",
    "\n",
    "# Find the best C (highest accuracy, smallest C in case of tie)\n",
    "best_accuracy = max(accuracy_scores.values())\n",
    "best_C_candidates = [C for C, acc in accuracy_scores.items() if acc == best_accuracy]\n",
    "best_C = min(best_C_candidates)\n",
    "\n",
    "print(f\"Best C (smallest C for best accuracy): {best_C}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
